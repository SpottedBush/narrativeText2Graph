{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_sample = \"Alice dropped her phone. She picked it up and smiled. \"\n",
    "\n",
    "sample = \"Dr. Glenn Tyler (Elvis Presley), a childish 25-year old, gets into a fight with and badly injures his drunken brother. A court releases him on probation into the care of his uncle in a small town, appointing Irene Sperry (Hope Lange) to give him psychological counselling. Marked as a trouble-maker, he is falsely suspected of various misdemeanors including an affair with Irene. Eventually shown to be innocent, he leaves to go to college and become a writer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0e740",
   "metadata": {},
   "source": [
    "### GLINER2 Spacy Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from preprocess import create_gliner_component\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\n",
    "    \"gliner-ner\",\n",
    "    name=\"gliner-ner\",\n",
    "    last=True,\n",
    "    config={\n",
    "        \"gliner_model\": \"fastino/gliner2-base-v1\",\n",
    "        \"entities\": [\"location\", \"fictionnal character\", \"actors\"],\n",
    "        \"threshold\": 0.5,\n",
    "        \"gpu\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample)\n",
    "doc._.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2675c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gliner2 import GLiNER2\n",
    "\n",
    "entities_to_extract = [\"location\", \"fictionnal character\", \"actors\"]\n",
    "\n",
    "exctractor = GLiNER2.from_pretrained(\"fastino/gliner2-base-v1\").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "out = exctractor.extract_entities(\n",
    "    text=doc.text,\n",
    "    entity_types=entities_to_extract,\n",
    "    threshold=0.5,\n",
    "    include_confidence=True,\n",
    ") # -> Dict[str, Any]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30023ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_occurrences(text: str, pattern: str):\n",
    "    \"\"\"Return all start indexes where `pattern` appears in `text`.\"\"\"\n",
    "    indexes = []\n",
    "    start = 0\n",
    "\n",
    "    while True:\n",
    "        idx = text.find(pattern, start)\n",
    "        if idx == -1:\n",
    "            break\n",
    "        indexes.append((idx, idx + len(pattern)))\n",
    "        start = idx + 1  # move forward to avoid infinite loops\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "all_occurences = []\n",
    "for ent in out[\"entities\"][\"actors\"]:\n",
    "    occurences = find_all_occurrences(doc.text, ent)\n",
    "    print(f\"{occurences}, {ent} -> {doc.text[occurences[0][0]:occurences[0][1]]}\")\n",
    "    all_occurences.append(occurences)\n",
    "\n",
    "for ent in out[\"entities\"][\"fictionnal character\"]:\n",
    "    occurences = find_all_occurrences(doc.text, ent)\n",
    "    print(f\"{occurences}, {ent} -> {doc.text[occurences[0][0]:occurences[0][1]]}\")\n",
    "    all_occurences.append(occurences)\n",
    "\n",
    "for ent in out[\"entities\"][\"location\"]:\n",
    "    occurences = find_all_occurrences(doc.text, ent)\n",
    "    print(f\"{occurences}, {ent} -> {doc.text[occurences[0][0]:occurences[0][1]]}\")\n",
    "    all_occurences.append(occurences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d385c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maverick import Maverick\n",
    "\n",
    "# model = Maverick(\n",
    "# #   hf_name_or_path = \"sapienzanlp/maverick-mes-preco\",\n",
    "#   hf_name_or_path = \"sapienzanlp/maverick-mes-ontonotes\",\n",
    "# #   device = \"cuda\"\n",
    "# )\n",
    "model = Maverick(hf_name_or_path=\"sapienzanlp/maverick-mes-preco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, eos_indices, speakers, char_offsets = model.preprocess(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(char_offsets)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbee753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_span_to_word_span(char_span: list[tuple], char_offsets: list[tuple]):\n",
    "    \"\"\"\n",
    "    char_span = (start char index, end char index) for entities (character indices)\n",
    "    char_offset = (word start char index, word end char index) for eah word (char to word map)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for entity_start_char_idx, entity_end_char_idx in char_span:\n",
    "        word_start_idx = -1\n",
    "        word_end_idx = len(char_offsets)\n",
    "        for idx, (word_start_char_idx, word_end_char_idx) in enumerate(char_offsets):\n",
    "            if word_start_char_idx == entity_start_char_idx:\n",
    "                word_start_idx = idx\n",
    "            if word_end_char_idx == entity_end_char_idx - 1: # Inclusive outer boundary\n",
    "                word_end_idx = idx\n",
    "            \n",
    "            if word_start_idx > -1 and word_end_idx < len(char_offsets):\n",
    "                break\n",
    "        \n",
    "        res.append((word_start_idx, word_end_idx))\n",
    "    return res\n",
    "\n",
    "            \n",
    "entities_word_format = []\n",
    "for occurence in all_occurences:\n",
    "    entities_word_format.append(char_span_to_word_span(occurence, char_offsets))\n",
    "\n",
    "for char_poses, word_poses in zip(all_occurences, entities_word_format):\n",
    "    for char_pos, word_pos in zip(char_poses, word_poses):\n",
    "        print(f\"{char_pos} -> {doc.text[char_pos[0]: char_pos[1]]} || {word_pos} -> {' '.join(tokens[word_pos[0]: word_pos[1] + 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fc458",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(doc.text, add_gold_clusters=entities_word_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17f90a",
   "metadata": {},
   "source": [
    "### GLINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89601d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "# Initialize GLiNER with the base model\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a517bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap, fill\n",
    "\n",
    "# Perform entity prediction\n",
    "entities = model.predict_entities(sample, entities_to_extract, threshold=0.5)\n",
    "\n",
    "print(fill(sample), \"\\n\")\n",
    "# Display predicted entities and their labels\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925a5e1",
   "metadata": {},
   "source": [
    "### FastCoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01877e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcoref import FCoref, spacy_component\n",
    "import json\n",
    "from preprocess import create_gliner_component\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\n",
    "    \"fastcoref\",\n",
    "    config={\n",
    "        \"model_architecture\": \"FCoref\",\n",
    "        \"model_path\": \"biu-nlp/f-coref\",\n",
    "        \"device\": 0,\n",
    "    },\n",
    ")\n",
    "nlp.add_pipe(\n",
    "    \"gliner-ner\",\n",
    "    name=\"gliner-ner\",\n",
    "    last=True,\n",
    "    config={\n",
    "        \"gliner_model\": \"fastino/gliner2-base-v1\",\n",
    "        \"entities\": [\"location\", \"fictionnal character\", \"actors\"],\n",
    "        \"threshold\": 0.5,\n",
    "        \"gpu\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample,\n",
    "             component_cfg={\"fastcoref\": {'resolve_text': True}})\n",
    "\n",
    "print(\"\\n\", fill(doc.text), \"\\n\\n\", fill(doc._.resolved_text))\n",
    "doc._.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cfacd",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429829dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "# pipe = stanza.Pipeline(\"en\", processors=\"tokenize, coref\", package={\"ner\": [\"CoNLL03\", \"ontonotes\"]})\n",
    "# pipe = stanza.Pipeline(\"en\", processors=\"tokenize, coref\", package={\"ner\": [\"ontonotes\"]})\n",
    "pipe = stanza.Pipeline(\"en\", processors=\"tokenize, coref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da862978",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pipe(sample)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ddacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in out.iter_words():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def coref_mentions(doc):\n",
    "    \n",
    "#     corefs = dict()\n",
    "    \n",
    "#     # Iter Words\n",
    "#     for word in doc.iter_words():\n",
    "#         word_dict = word.to_dict()\n",
    "        \n",
    "#         # Iter Corefs\n",
    "#         for chain in word_dict.get(\"coref_chains\", []):\n",
    "#             chain = chain.to_json()\n",
    "#             entity_ndx = chain.get(\"index\")\n",
    "#             # First time to see this entity\n",
    "#             if not corefs.get(entity_ndx):\n",
    "#                 corefs[entity_ndx] = []\n",
    "\n",
    "#             corefs[entity_ndx].append(word_dict.get(\"text\"))\n",
    "\n",
    "    \n",
    "#     return corefs\n",
    "\n",
    "def coref_mentions(doc):\n",
    "    # Final result dictionary: {entity_index: [\"Full Entity Name\", \"Another Mention\"]}\n",
    "    corefs = dict()\n",
    "    \n",
    "    # Temporary buffer to hold words while a mention is being built\n",
    "    # Format: {entity_index: [\"Dr.\", \"Glenn\"]}\n",
    "    active_spans = dict()\n",
    "    active_positions = dict()\n",
    "    \n",
    "    # Iter Words\n",
    "    for word in doc.iter_words():\n",
    "        word_dict = word.to_dict()\n",
    "        word_text = word_dict.get(\"text\")\n",
    "        word_pos_start = word_dict.get(\"start_char\")\n",
    "        word_pos_end = word_dict.get(\"end_char\")\n",
    "        \n",
    "        # Iter Corefs\n",
    "        for chain in word_dict.get(\"coref_chains\", []):\n",
    "            # Ensure chain is a dict\n",
    "            if not isinstance(chain, dict):\n",
    "                chain = chain.to_json()\n",
    "                \n",
    "            entity_ndx = chain.get(\"index\")\n",
    "            is_start = chain.get(\"is_start\", False)\n",
    "            is_end = chain.get(\"is_end\", False)\n",
    "\n",
    "            # 1. Start of a new mention span\n",
    "            if is_start:\n",
    "                active_spans[entity_ndx] = []\n",
    "                active_positions[entity_ndx] = []\n",
    "\n",
    "            # 2. Add current word to the active buffer if we are tracking this index\n",
    "            if entity_ndx in active_spans:\n",
    "                active_spans[entity_ndx].append(word_text)\n",
    "                active_positions[entity_ndx].extend([word_pos_start, word_pos_end])\n",
    "\n",
    "            # 3. End of the mention span\n",
    "            if is_end:\n",
    "                if entity_ndx in active_spans:\n",
    "                    # Combine the buffered words into a single string\n",
    "                    full_entity_text = \" \".join(active_spans[entity_ndx])\n",
    "                    full_entity_start = min(active_positions[entity_ndx])\n",
    "                    full_entity_end = max(active_positions[entity_ndx])\n",
    "                    \n",
    "                    # Initialize list in final dict if not present\n",
    "                    if entity_ndx not in corefs:\n",
    "                        corefs[entity_ndx] = []\n",
    "                    \n",
    "                    # Add the full string to results\n",
    "                    corefs[entity_ndx].append((full_entity_text, full_entity_start, full_entity_end))\n",
    "                    \n",
    "                    # Remove from active spans so we don't keep appending to it\n",
    "                    del active_spans[entity_ndx]\n",
    "                    del active_positions[entity_ndx]\n",
    "\n",
    "    return corefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_mentions(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b81ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "825f577f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a02756",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ccde6c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess\n",
    "import json\n",
    "\n",
    "out = preprocess(sample)\n",
    "print(json.dumps(out[\"sentences\"], indent=2, ensure_ascii=False))\n",
    "doc = out[\"doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from extraction import (\n",
    "    detect_entities,\n",
    "    detect_events,\n",
    "    create_narrative_segment,\n",
    "    merge_narrative_segments,\n",
    ")\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "graph = None\n",
    "segs = []\n",
    "for sentence in out[\"sentences\"]:\n",
    "    print(f\"- {sentence}\")\n",
    "    doc = nlp(sentence)\n",
    "    ents = detect_entities(doc)\n",
    "    evs = detect_events(doc)\n",
    "    curr_seg_nodes, graph = create_narrative_segment(ents, evs, graph=graph)\n",
    "    segs.append({\"nodes\": curr_seg_nodes, \"graph\": graph})\n",
    "    print(f\"  Entities: {len(ents)}, Events: {len(evs)}\")\n",
    "    print(f\"  Entities: {ents}\")\n",
    "    print(f\"  Events: {evs}\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafff140",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ents:\n",
    "    print(\"Entity Text:\", text[\"text\"])\n",
    "    print(text[\"label\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b45ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in evs:\n",
    "    print(\"Event Text:\", text[\"text\"])\n",
    "    print(\"Subject Texts:\", text[\"subject_texts\"])\n",
    "    print(\"Object Texts:\", text[\"object_texts\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_graph\n",
    "\n",
    "curr_nodes, curr_graph = segs[0][\"nodes\"], segs[0][\"graph\"]\n",
    "visualize_graph(curr_graph, nodes=curr_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba199f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrativetext2graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
